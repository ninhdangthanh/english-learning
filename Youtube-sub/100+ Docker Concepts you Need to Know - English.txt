
Welcome to Docker 101! If your goal is to ship software in the real world, one of the most powerful concepts to understand is containerization. When developing locally, it solves the age-old problem of "it works on my machine," and when deploying in the cloud, it solves the age-old problem of "this architecture doesn't scale."

Over the next few minutes, we'll unlock the power inside this container by learning 101 different concepts and terms related to computer science, the cloud, and, of course, Docker.

I'm guessing you know what a computer is, right? It's a box that has three important components inside: a CPU for calculating things, random access memory for the applications you're using right now, and a disk to store things you might use later. This is bare metal hardware, but in order to use it, we need an operating system. Most importantly, the OS provides a kernel that sits on top of the bare metal, allowing software applications to run on it.

In the olden days, you would go to the store and buy software to physically install it on your machine. But nowadays, most software is delivered via the Internet through the magic of networking. When you watch a YouTube video, your computer is called the client. You and billions of other users are getting that data from remote computers called servers.

When an app starts reaching millions of people, weird things begin to happen. The CPU becomes exhausted handling all the incoming requests, the disk slows down, network bandwidth gets maxed out, and the database becomes too large to query effectively. On top of that, you might have written some garbage code that's causing race conditions, memory leaks, and unhandled errors that will eventually grind your server to a halt.

The big question is: How do we scale our infrastructure? A server can scale up in two ways: vertically or horizontally. To scale vertically, you take your one server and increase its RAM and CPU. This can take you pretty far, but eventually, you hit a ceiling. The other option is to scale horizontally, where you distribute your code to multiple smaller servers, which are often broken down into microservices that can run and scale independently.

But distributed systems like this aren't very practical when talking about bare metal because actual resource allocation varies. One way engineers address this is with virtual machines. Using tools like hypervisors, you can isolate and run multiple operating systems on a single machine. That helps, but a VM's allocation of CPU and memory is still fixed, and that's where Docker comes in, the sponsor of today's video.

Applications running on top of the Docker engine all share the same host operating system kernel and use resources dynamically based on their needs. Under the hood, Docker is running a daemon or persistent process that makes all this magic possible and gives us OS-level virtualization. What's awesome is that any developer can easily harness this power by simply installing Docker Desktop. It allows you to develop software without having to make massive changes to your local system.

Here's how Docker works in three easy steps:

1. **First**, you start with a Dockerfile. This is like a blueprint that tells Docker how to configure the environment that runs your application. The Dockerfile is then used to build an image, which contains an OS, your dependencies, and your code—like a template for running your application. We can upload this image to the cloud to places like Docker Hub and share it with the world. But an image by itself doesn't do anything; you need to run it as a container, which is an isolated package running your code that, in theory, could scale infinitely in the cloud. Containers are stateless, which means when they shut down, all the data inside them is lost. But that makes them portable, and they can run on every major cloud platform without vendor lock-in.

2. **Next**, the best way to learn Docker is to actually run a container. Let's do that right now by creating a Dockerfile. A Dockerfile contains a collection of instructions, which by convention are in all caps. `FROM` is usually the first instruction you'll see, which points to a base image to get started. This will often be a Linux distro and may be followed by a colon, which is an optional image tag specifying the version of the OS. Next, we have the `WORKDIR` instruction, which creates a source directory and changes into it. That's where we'll put our source code. All commands from here on out will be executed from this working directory. Next, we use the `RUN` instruction to use a Linux package manager to install our dependencies. `RUN` lets you execute any command just like you would from the command line. Currently, we're running as the root user, but for better security, we could also create a non-root user with the `USER` instruction. Now we use `COPY` to copy the code on our local machine over to the image. You're halfway there! Let's take a brief intermission.

3. **To run this code**, we need an API key, which we can set as an environment variable with the `ENV` instruction. We're building a web server that people can connect to, which requires a port for external traffic. Use the `EXPOSE` instruction to make that port accessible. Finally, that brings us to the `CMD` instruction, which is the command you want to run when starting up a container. In this case, it will run our web server. There can only be one command per container, although you might also add an `ENTRYPOINT`, allowing you to pass arguments to the command when you run it. That's everything we need for the Dockerfile, but as an added touch, we could also use `LABEL` to add some extra metadata, or run a health check to make sure it's running properly. If the container needs to store data that's going to be used later or by multiple containers, we could mount a volume to it with a persistent disk.

**Okay, we have a Dockerfile—so now what?** When you install Docker Desktop, it also installs the Docker CLI, which you can run from the terminal. Run `docker help` to see all the possible commands, but the one we need right now is `docker build`, which will turn this Dockerfile into an image. When you run the command, it's a good idea to use the `-t` flag to tag it with a recognizable name. Notice how it builds the image in layers. Every layer is identified by a SHA-256 hash, which means if you modify your Dockerfile, each layer will be cached so it only has to rebuild what is actually changed. That makes your workflow as a developer far more efficient. In addition, it's important to point out that sometimes you don't want certain files to end up in a Docker image. In which case, you can add them to the `.dockerignore` file to exclude them from the actual files that get copied there.

**Now, open Docker Desktop and view the image there.** Not only does it give us a detailed breakdown, but thanks to Docker Scout, we're able to proactively identify any security vulnerabilities for each layer of the image. It works by extracting the software bill of material from the image and comparing it to a bunch of security advisory databases. When there's a match, it's given a severity rating so you can prioritize your security efforts.

**But now the time has finally come to run a container.** We can accomplish that by simply clicking on the "Run" button. Under the hood, it executes the `docker run` command, and we can now access our server on localhost. In addition, we can see the running container here in Docker Desktop, which is the equivalent of the `docker` command you can run from the terminal to get a breakdown of all the running and stopped containers on your machine. If we click on it, we can inspect the logs from this container, view the file system, and even execute commands directly inside the running container.

**When it comes time to shut it down**, we can use `docker stop` to stop it gracefully, or `docker kill` to forcefully stop it. You can still see the shutdown container here in the UI or use `docker rm` to get rid of it. But now you might want to run your container in the cloud. `docker push` will upload your image to a remote registry, where it can then run on a cloud like AWS with Elastic Container Service or be launched on serverless platforms like Google Cloud Run. Conversely, you may want to use someone else's Docker image, which can be downloaded from the cloud with `docker pull`. Now you can run any developer's code without having to make any changes to your local environment or machine.

**Congratulations! You're now a bona fide and certified Docker expert.** I hereby grant you permission to print out this certificate and bring it to your next job interview. But Docker itself is only the beginning. There's a good chance your application has more than one service, in which case you'll want to know about Docker Compose, a tool for managing multi-container applications. It allows you to define multiple applications and their Docker images in a single YAML file, like a frontend, a backend, and a database. The `docker compose up` command will spin up all the containers simultaneously, while the `down` command will stop them. That works on an individual server, but once you reach massive scale, you'll likely need an orchestration tool like Kubernetes to run and manage containers all over the world.

**It works like this:** You have a control plane that exposes an API to manage the cluster. Now, the cluster has multiple nodes or machines, each one containing a kubelet and multiple pods. A pod is the minimum deployable unit in Kubernetes, which itself has one or more containers inside it. What makes Kubernetes so effective is that you can describe the desired state of the system, and it will automatically scale up or scale down while also providing fault tolerance to automatically heal if one of your servers goes down

. It gets pretty complicated, but the good news is that you probably don't need Kubernetes. It was developed at Google based on its Borg system and is really only necessary for highly complex, high-traffic systems. If that sounds like you, though, you can also use extensions on Docker Desktop to debug your pods.

With that, we've looked at 100 concepts related to containerization. Big shout-out to Docker for making this video possible. Thanks for watching, and I will see you in the next one!

---
