
The hardest aspect to scale in a web application is the database. Therefore, to improve performance, a cache layer is often added to the stack. This can be quite effective in enhancing performance while reducing the number of database queries. However, in software development, nothing comes without a cost, and caching introduces its own set of challenges. The most significant issue is stale data, which occurs when the data in your cache no longer aligns with what’s in your database. This problem is addressed through cache invalidation, but knowing when to flush your cache can be quite difficult. The difficulty often depends on the type of data you’re caching and the pattern you’re using.

To understand these challenges and how to overcome them, let's explore implementing the most common caching pattern: cache-aside, also known as lazy loading. In this strategy, your application will initially request data from the cache. If there’s a hit, the data is returned to the client. If the cache misses, however, the database is queried instead. The data is then both stored in the cache and returned to the caller. It’s a relatively simple strategy, but it works quite effectively.

To see this in action, let’s go ahead and implement it. To accompany this video, I’ve created a base project in Rust, which is a simple CRUD app for managing spells inside a wizard's spellbook. The tech stack of this project includes Rust and Axum for the HTTP server. For the database, we’re using PostgreSQL with SQLx, and for the cache, I’ve chosen Redis. The actual tech stack doesn’t matter too much; you could easily replace Redis with an in-memory cache if you prefer. However, the benefit of using Redis is that we can view the data it contains using the CLI.

To implement the cache-aside strategy, first, clone the project files using Git. Once completed, you can open the project in your favorite text editor and navigate to the `handlers/read.rs` file. Inside, there’s a `find_by_id` function used by our HTTP handler to pull out spells from the database by their ID. Before we add caching, let’s see this endpoint in action. To run this, we’ll need an instance of Postgres. You can run this locally if you prefer, but to give this a more real-world feel, we’ll use the sponsor of today’s video, Aiven, which allows us to deploy both a Postgres and Redis instance for free.

To deploy our instance, head over to the Aiven website at `go.aiven.io/dreamsofcode`, or click the link in the description below. Create a free Aiven account using your preferred method. Once signed up, create a new project for the application. For me, I’m going to call this “Spellwork.” With our project created, we’re then prompted to create a new service. We want to create a Postgres database, so go ahead and click the service button. Upon doing so, make sure to select the free plan and choose the location closest to you. Finally, give your service a name and click the big blue "Create" button. You’ll then be shown your connection details. Copy the service URI to your clipboard, then head back to your project files. Inside the project, you’ll find an `.env` file with a couple of environment variables left empty. Paste the service URI into the `DATABASE_URL` environment variable.

Afterwards, we can run our server using the `cargo run` command. This will automatically perform a database migration for us and insert eight rows into the spells table as well. If we send a curl request to the `localhost:3000/spells` endpoint, we can retrieve all the rows in our table. The handler we looked at, which we’re about to add caching to, is the `/spells/:id` endpoint, which allows us to pull out individual spells by their ID. We can measure the amount of time the request takes by heading over to the `main.rs` file and enabling the debug logging level. Then, if we rerun our server and send two `find_by_id` requests, we can see that it takes around 100 milliseconds for the request to process. You’ll notice the first request was slightly longer; that’s because it’s creating a prepared statement that is then reused by Postgres.

Let’s take a mental note of how long this took so we can compare it to the cache-aside version we’re about to implement. To do so, navigate back to your editor and go to the `handlers/read.rs` file. Inside the `find_by_id` function is where we’re going to implement this strategy. The first step is to check the cache with the ID of the spell we’re looking for to see if it contains an entry. In our case, the Redis connection (or cache) is stored inside the AppState. Therefore, we can use the `get` method, passing in the ID of the spell. This is what that would look like in the Redis CLI.

Since this method can fail, we don’t want it to break the execution flow of the function. Therefore, we’ll use the `unwrap_or` method to return `None` if it’s an error. On success, this method returns an optional spell, representing whether or not there was an entry inside the cache. We can then use the following lines to check if the spell exists, and if so, return it to the client. If the result is `None`, this will continue with the existing workflow. This is the first half of cache-aside. The second half is that we store the result from the database inside the cache. First, we’ll check to see if we received a result from the database. If so, we can then write it to the cache using the `set` method, passing in the ID and the spell. This method also expects a couple more parameters; we’ll set these to be `None` and `false` for now.

With that, we’re writing to Redis as if we were doing the following command in the Redis CLI. We now have a basic implementation of cache-aside. However, before we can run this code, we need a Redis instance that we can connect to. Fortunately, Aiven has us covered again, as they provide a free version of Redis that we can use. To do so, head back to the Aiven dashboard and create a new Redis service. Again, be sure to select the free version and make sure it’s in a location close to you, preferably in the same place as Postgres. Once deployed, go ahead and copy the connection URI, then head back to the `.env` file and add it to the `REDIS_URL` environment variable.

With that, we’re ready to run our code again using the `cargo run` command. When we send our first request to the `/spells/:id` endpoint, you’ll notice in the logs that we’re retrieving the version from the database. However, if we make the same request again, this time it’s coming from the cache. If we re-enable debug logging to measure the time the request takes, we can see that the response that pulls from the database takes about 250 milliseconds, with the cached response taking around 50 milliseconds. Therefore, by using this cache, we’ve reduced the time it takes to pull out a record by 50% in the best case. However, in the worst case, we’ve actually increased it by two-thirds. That’s because, instead of performing a single request to the database, we’re now performing three requests in total, with the additional two going toward Redis.

Fortunately, there’s a way to improve this performance by concurrently writing to the cache and sending the result back to the client. We can do this in our code by using the following lines, which will perform our cache write in a spawned Tokio task. Because this task occurs concurrently, the return of the result doesn’t have to wait for the write to finish. Now, when we rerun our code, we can see that our first request (the worst case) only takes an extra 50 milliseconds compared to before. So while there is still a performance hit, we’ve managed to reduce its impact.

By the way, you may have noticed that every time we restart the server, it flushes all the keys inside the cache. This was done to potentially simplify development during this first section, but you likely wouldn’t want to do this in production, especially if you have multiple instances reading from the same cache. However, removing this presents a problem with our implementation: cache invalidation, or the lack thereof. Essentially, our keys will remain in the cache for as long as it exists. Therefore, to prevent our cache from consuming all of the available memory, we need to find a way to invalidate our keys.

Fortunately, we have a couple of options. The first is to use an eviction policy, which tells Redis how to remove keys from the cache when memory pressure climbs too high. If we head back to our Redis instance in Aiven, we can select one of the policies we want to use. Let’s take a look at what each one does. The first of these policies is "evict or keys least recently used first" (LRU). This policy frees up memory by removing keys in the cache starting with those that were least recently used, with "used" being defined as either written to or read from.

The next policy is "evict only keys with expire set, least recently used first." This is similar to the above but will only evict keys if they have an expiration set (we’ll talk more about that shortly). Next, we have "evict all keys in random order," which is essentially a form of chaos. The next one is similar but will only evict keys if they have an expiration set. Below this, we have "evict only
